{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure Computer Vision: Image Analysis 4.0\n",
    "\n",
    "[Quickstart: Image Analysis 4.0](https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/quickstarts-sdk/image-analysis-client-library-40?tabs=visual-studio%2Cwindows&pivots=programming-language-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "* [Azure Computer Vision](https://portal.azure.com/#create/Microsoft.CognitiveServicesComputerVision)\n",
    "* Python environment, version 3.10 or higher\n",
    "* Visual Studio Code\n",
    "  * Extensions: Python and Jupyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python packages\n",
    "#! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "from azure.core.credentials import AzureKeyCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load variables\n",
    "load_dotenv()\n",
    "\n",
    "# Azure Python libraries\n",
    "endpoint = os.environ[\"AZURE_AI_MULTISERVICE_ENDPOINT\"]\n",
    "subscription_key = os.environ[\"AZURE_AI_MULTISERVICE_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To authenticate against the Image Analysis service, you need a Computer Vision key and endpoint URL. This guide assumes that you've defined the environment variables VISION_KEY and VISION_ENDPOINT with your key and endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Image Analysis client\n",
    "# Python ImageAnalysisClient Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.imageanalysisclient?view=azure-python\n",
    "client = ImageAnalysisClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(subscription_key)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the Image Analysis results, the following prerequisites needs to be completed:\n",
    "* Select the image to analyze\n",
    "  * You can select an image by providing a publicly accessible image URL, or by [reading image data into the SDK's input buffer](https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/call-analyze-image-40?pivots=programming-language-python#image-buffer).\n",
    "* Select visual features\n",
    "* Optionally configure Select smart cropping aspect ratios, gender_neutral_caption and language when sending the analyzed image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image to analyze into a 'bytes' object\n",
    "with open(\"../images-lab-tests/seattle-pikeplace-1.jpg\", \"rb\") as f:\n",
    "    image_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image analysis results:\n",
      "\n",
      " Captions:\n",
      "   'a group of cars parked outside of a market with Pike Place Market in the background', Confidence 0.6851\n",
      "\n",
      " Dense captions:\n",
      "   'a group of cars parked outside of a market', Confidence 0.6851, {'x': 0, 'y': 0, 'w': 700, 'h': 500}\n",
      "   'a clock with red hands', Confidence 0.7682, {'x': 278, 'y': 105, 'w': 75, 'h': 74}\n",
      "   'a close up of a car', Confidence 0.8497, {'x': 112, 'y': 315, 'w': 197, 'h': 65}\n",
      "   'a car parked in a parking lot', Confidence 0.7255, {'x': 0, 'y': 331, 'w': 275, 'h': 95}\n",
      "   'a car parked on a brick road', Confidence 0.7291, {'x': 19, 'y': 342, 'w': 648, 'h': 152}\n",
      "   'a sign with a clock on it', Confidence 0.7288, {'x': 68, 'y': 47, 'w': 281, 'h': 185}\n",
      "\n",
      " Read-OCR:\n",
      "   Line: 'PUBLIC', Bounding box [{'x': 126, 'y': 59}, {'x': 313, 'y': 59}, {'x': 314, 'y': 106}, {'x': 127, 'y': 111}]\n",
      "     Word: 'PUBLIC', Bounding polygon [{'x': 126, 'y': 60}, {'x': 304, 'y': 62}, {'x': 302, 'y': 102}, {'x': 128, 'y': 112}], Confidence 0.8800\n",
      "   Line: 'MEET THE PRODUCE', Bounding box [{'x': 447, 'y': 60}, {'x': 699, 'y': 75}, {'x': 698, 'y': 97}, {'x': 445, 'y': 81}]\n",
      "     Word: 'MEET', Bounding polygon [{'x': 447, 'y': 61}, {'x': 512, 'y': 64}, {'x': 511, 'y': 86}, {'x': 446, 'y': 82}], Confidence 0.9880\n",
      "     Word: 'THE', Bounding polygon [{'x': 529, 'y': 65}, {'x': 571, 'y': 67}, {'x': 570, 'y': 90}, {'x': 528, 'y': 87}], Confidence 0.9940\n",
      "     Word: 'PRODUCE', Bounding polygon [{'x': 587, 'y': 68}, {'x': 696, 'y': 76}, {'x': 696, 'y': 98}, {'x': 586, 'y': 91}], Confidence 0.9950\n",
      "   Line: 'MARKET', Bounding box [{'x': 76, 'y': 119}, {'x': 280, 'y': 119}, {'x': 280, 'y': 173}, {'x': 76, 'y': 173}]\n",
      "     Word: 'MARKET', Bounding polygon [{'x': 77, 'y': 121}, {'x': 279, 'y': 119}, {'x': 279, 'y': 174}, {'x': 76, 'y': 172}], Confidence 0.9900\n",
      "   Line: '....', Bounding box [{'x': 439, 'y': 123}, {'x': 461, 'y': 123}, {'x': 460, 'y': 130}, {'x': 439, 'y': 129}]\n",
      "     Word: '....', Bounding polygon [{'x': 439, 'y': 123}, {'x': 461, 'y': 125}, {'x': 459, 'y': 130}, {'x': 439, 'y': 129}], Confidence 0.9430\n",
      "   Line: '..', Bounding box [{'x': 543, 'y': 128}, {'x': 558, 'y': 129}, {'x': 558, 'y': 134}, {'x': 543, 'y': 133}]\n",
      "     Word: '..', Bounding polygon [{'x': 546, 'y': 129}, {'x': 554, 'y': 129}, {'x': 553, 'y': 134}, {'x': 545, 'y': 133}], Confidence 0.7960\n",
      "   Line: 'CENTER', Bounding box [{'x': 122, 'y': 182}, {'x': 318, 'y': 185}, {'x': 317, 'y': 230}, {'x': 122, 'y': 228}]\n",
      "     Word: 'CENTER', Bounding polygon [{'x': 123, 'y': 183}, {'x': 312, 'y': 190}, {'x': 310, 'y': 228}, {'x': 123, 'y': 230}], Confidence 0.9930\n",
      "   Line: 'FARMERS MARKET', Bounding box [{'x': 356, 'y': 228}, {'x': 599, 'y': 229}, {'x': 599, 'y': 253}, {'x': 356, 'y': 252}]\n",
      "     Word: 'FARMERS', Bounding polygon [{'x': 356, 'y': 229}, {'x': 477, 'y': 229}, {'x': 477, 'y': 253}, {'x': 356, 'y': 253}], Confidence 0.9940\n",
      "     Word: 'MARKET', Bounding polygon [{'x': 493, 'y': 229}, {'x': 600, 'y': 230}, {'x': 599, 'y': 254}, {'x': 493, 'y': 253}], Confidence 0.9950\n",
      "   Line: 'Seattle, Washington', Bounding box [{'x': 18, 'y': 403}, {'x': 692, 'y': 406}, {'x': 691, 'y': 473}, {'x': 18, 'y': 468}]\n",
      "     Word: 'Seattle,', Bounding polygon [{'x': 19, 'y': 410}, {'x': 271, 'y': 404}, {'x': 271, 'y': 471}, {'x': 19, 'y': 466}], Confidence 0.9940\n",
      "     Word: 'Washington', Bounding polygon [{'x': 284, 'y': 404}, {'x': 674, 'y': 411}, {'x': 674, 'y': 474}, {'x': 284, 'y': 471}], Confidence 0.9940\n",
      "\n",
      " Objects:\n",
      "  Object 1:\n",
      "   'boundingBox: {'x': 0, 'y': 326, 'w': 300, 'h': 97}'\n",
      "   'tags: [{'name': 'car', 'confidence': 0.777}]'\n",
      "\n",
      " People:\n",
      "  Person 1:\n",
      "   'boundingBox: {'x': 490, 'y': 304, 'w': 22, 'h': 48}'\n",
      "   'confidence: 0.746627688407898'\n",
      "  Person 2:\n",
      "   'boundingBox: {'x': 2, 'y': 308, 'w': 26, 'h': 55}'\n",
      "   'confidence: 0.7361218333244324'\n",
      "  Person 3:\n",
      "   'boundingBox: {'x': 570, 'y': 310, 'w': 17, 'h': 43}'\n",
      "   'confidence: 0.7098609805107117'\n",
      "  Person 4:\n",
      "   'boundingBox: {'x': 509, 'y': 306, 'w': 19, 'h': 44}'\n",
      "   'confidence: 0.6827504634857178'\n",
      "  Person 5:\n",
      "   'boundingBox: {'x': 549, 'y': 306, 'w': 14, 'h': 45}'\n",
      "   'confidence: 0.6727105975151062'\n",
      "  Person 6:\n",
      "   'boundingBox: {'x': 593, 'y': 307, 'w': 15, 'h': 47}'\n",
      "   'confidence: 0.6222187280654907'\n",
      "  Person 7:\n",
      "   'boundingBox: {'x': 539, 'y': 311, 'w': 13, 'h': 41}'\n",
      "   'confidence: 0.5760930776596069'\n",
      "  Person 8:\n",
      "   'boundingBox: {'x': 395, 'y': 299, 'w': 16, 'h': 46}'\n",
      "   'confidence: 0.42922109365463257'\n",
      "  Person 9:\n",
      "   'boundingBox: {'x': 530, 'y': 301, 'w': 13, 'h': 45}'\n",
      "   'confidence: 0.2550559937953949'\n",
      "  Person 10:\n",
      "   'boundingBox: {'x': 355, 'y': 304, 'w': 15, 'h': 40}'\n",
      "   'confidence: 0.14587226510047913'\n",
      "  Person 11:\n",
      "   'boundingBox: {'x': 181, 'y': 305, 'w': 16, 'h': 13}'\n",
      "   'confidence: 0.13202686607837677'\n",
      "  Person 12:\n",
      "   'boundingBox: {'x': 521, 'y': 307, 'w': 16, 'h': 40}'\n",
      "   'confidence: 0.1216173842549324'\n",
      "  Person 13:\n",
      "   'boundingBox: {'x': 83, 'y': 310, 'w': 19, 'h': 21}'\n",
      "   'confidence: 0.11294268071651459'\n",
      "  Person 14:\n",
      "   'boundingBox: {'x': 134, 'y': 307, 'w': 14, 'h': 14}'\n",
      "   'confidence: 0.10997774451971054'\n",
      "  Person 15:\n",
      "   'boundingBox: {'x': 245, 'y': 308, 'w': 11, 'h': 27}'\n",
      "   'confidence: 0.10772374272346497'\n",
      "  Person 16:\n",
      "   'boundingBox: {'x': 584, 'y': 314, 'w': 11, 'h': 39}'\n",
      "   'confidence: 0.10394099354743958'\n",
      "  Person 17:\n",
      "   'boundingBox: {'x': 164, 'y': 308, 'w': 9, 'h': 10}'\n",
      "   'confidence: 0.09208350628614426'\n",
      "  Person 18:\n",
      "   'boundingBox: {'x': 188, 'y': 305, 'w': 17, 'h': 12}'\n",
      "   'confidence: 0.09178169816732407'\n",
      "  Person 19:\n",
      "   'boundingBox: {'x': 147, 'y': 308, 'w': 10, 'h': 11}'\n",
      "   'confidence: 0.09157565981149673'\n",
      "  Person 20:\n",
      "   'boundingBox: {'x': 460, 'y': 300, 'w': 13, 'h': 42}'\n",
      "   'confidence: 0.08697645366191864'\n",
      "  Person 21:\n",
      "   'boundingBox: {'x': 114, 'y': 303, 'w': 18, 'h': 26}'\n",
      "   'confidence: 0.06524883210659027'\n",
      "  Person 22:\n",
      "   'boundingBox: {'x': 107, 'y': 307, 'w': 19, 'h': 26}'\n",
      "   'confidence: 0.05664151906967163'\n",
      "  Person 23:\n",
      "   'boundingBox: {'x': 16, 'y': 309, 'w': 23, 'h': 50}'\n",
      "   'confidence: 0.04714203625917435'\n",
      "  Person 24:\n",
      "   'boundingBox: {'x': 448, 'y': 299, 'w': 13, 'h': 43}'\n",
      "   'confidence: 0.04432445019483566'\n",
      "  Person 25:\n",
      "   'boundingBox: {'x': 295, 'y': 308, 'w': 14, 'h': 37}'\n",
      "   'confidence: 0.04412062466144562'\n",
      "  Person 26:\n",
      "   'boundingBox: {'x': 280, 'y': 304, 'w': 10, 'h': 22}'\n",
      "   'confidence: 0.0439797081053257'\n",
      "  Person 27:\n",
      "   'boundingBox: {'x': 282, 'y': 308, 'w': 14, 'h': 34}'\n",
      "   'confidence: 0.03313422575592995'\n",
      "  Person 28:\n",
      "   'boundingBox: {'x': 307, 'y': 308, 'w': 9, 'h': 34}'\n",
      "   'confidence: 0.03309652954339981'\n",
      "  Person 29:\n",
      "   'boundingBox: {'x': 233, 'y': 303, 'w': 9, 'h': 9}'\n",
      "   'confidence: 0.030387727543711662'\n",
      "  Person 30:\n",
      "   'boundingBox: {'x': 0, 'y': 310, 'w': 7, 'h': 63}'\n",
      "   'confidence: 0.02437586523592472'\n",
      "  Person 31:\n",
      "   'boundingBox: {'x': 563, 'y': 304, 'w': 10, 'h': 25}'\n",
      "   'confidence: 0.020241187885403633'\n",
      "  Person 32:\n",
      "   'boundingBox: {'x': 97, 'y': 311, 'w': 13, 'h': 22}'\n",
      "   'confidence: 0.01837286539375782'\n",
      "  Person 33:\n",
      "   'boundingBox: {'x': 351, 'y': 303, 'w': 13, 'h': 33}'\n",
      "   'confidence: 0.017666146159172058'\n",
      "  Person 34:\n",
      "   'boundingBox: {'x': 532, 'y': 300, 'w': 13, 'h': 19}'\n",
      "   'confidence: 0.017041770741343498'\n",
      "  Person 35:\n",
      "   'boundingBox: {'x': 29, 'y': 316, 'w': 14, 'h': 39}'\n",
      "   'confidence: 0.016676902770996094'\n",
      "  Person 36:\n",
      "   'boundingBox: {'x': 264, 'y': 312, 'w': 13, 'h': 13}'\n",
      "   'confidence: 0.014095384627580643'\n",
      "  Person 37:\n",
      "   'boundingBox: {'x': 0, 'y': 304, 'w': 7, 'h': 33}'\n",
      "   'confidence: 0.012572374194860458'\n",
      "  Person 38:\n",
      "   'boundingBox: {'x': 79, 'y': 308, 'w': 9, 'h': 22}'\n",
      "   'confidence: 0.010956701822578907'\n",
      "  Person 39:\n",
      "   'boundingBox: {'x': 380, 'y': 299, 'w': 8, 'h': 18}'\n",
      "   'confidence: 0.01084492914378643'\n",
      "  Person 40:\n",
      "   'boundingBox: {'x': 53, 'y': 305, 'w': 15, 'h': 41}'\n",
      "   'confidence: 0.010741088539361954'\n",
      "  Person 41:\n",
      "   'boundingBox: {'x': 582, 'y': 307, 'w': 11, 'h': 29}'\n",
      "   'confidence: 0.01032678596675396'\n",
      "  Person 42:\n",
      "   'boundingBox: {'x': 392, 'y': 304, 'w': 10, 'h': 40}'\n",
      "   'confidence: 0.008126838132739067'\n",
      "  Person 43:\n",
      "   'boundingBox: {'x': 538, 'y': 301, 'w': 19, 'h': 16}'\n",
      "   'confidence: 0.008019288070499897'\n",
      "  Person 44:\n",
      "   'boundingBox: {'x': 23, 'y': 307, 'w': 39, 'h': 42}'\n",
      "   'confidence: 0.007549446541815996'\n",
      "  Person 45:\n",
      "   'boundingBox: {'x': 247, 'y': 310, 'w': 25, 'h': 18}'\n",
      "   'confidence: 0.0070360261015594006'\n",
      "  Person 46:\n",
      "   'boundingBox: {'x': 507, 'y': 307, 'w': 11, 'h': 28}'\n",
      "   'confidence: 0.006984697189182043'\n",
      "  Person 47:\n",
      "   'boundingBox: {'x': 263, 'y': 312, 'w': 17, 'h': 25}'\n",
      "   'confidence: 0.006751046050339937'\n",
      "  Person 48:\n",
      "   'boundingBox: {'x': 549, 'y': 299, 'w': 15, 'h': 20}'\n",
      "   'confidence: 0.006156621966511011'\n",
      "  Person 49:\n",
      "   'boundingBox: {'x': 200, 'y': 308, 'w': 10, 'h': 10}'\n",
      "   'confidence: 0.0059622726403176785'\n",
      "  Person 50:\n",
      "   'boundingBox: {'x': 523, 'y': 305, 'w': 31, 'h': 48}'\n",
      "   'confidence: 0.005085540469735861'\n",
      "  Person 51:\n",
      "   'boundingBox: {'x': 488, 'y': 303, 'w': 12, 'h': 24}'\n",
      "   'confidence: 0.005027612671256065'\n",
      "  Person 52:\n",
      "   'boundingBox: {'x': 349, 'y': 300, 'w': 9, 'h': 33}'\n",
      "   'confidence: 0.005012867972254753'\n",
      "  Person 53:\n",
      "   'boundingBox: {'x': 568, 'y': 307, 'w': 12, 'h': 14}'\n",
      "   'confidence: 0.005003722850233316'\n",
      "  Person 54:\n",
      "   'boundingBox: {'x': 128, 'y': 345, 'w': 19, 'h': 19}'\n",
      "   'confidence: 0.004683772101998329'\n",
      "  Person 55:\n",
      "   'boundingBox: {'x': 45, 'y': 303, 'w': 19, 'h': 36}'\n",
      "   'confidence: 0.004190901760011911'\n",
      "  Person 56:\n",
      "   'boundingBox: {'x': 58, 'y': 319, 'w': 9, 'h': 26}'\n",
      "   'confidence: 0.004114966839551926'\n",
      "  Person 57:\n",
      "   'boundingBox: {'x': 586, 'y': 301, 'w': 11, 'h': 20}'\n",
      "   'confidence: 0.0034219087101519108'\n",
      "  Person 58:\n",
      "   'boundingBox: {'x': 584, 'y': 301, 'w': 18, 'h': 44}'\n",
      "   'confidence: 0.002949815709143877'\n",
      "  Person 59:\n",
      "   'boundingBox: {'x': 231, 'y': 303, 'w': 12, 'h': 18}'\n",
      "   'confidence: 0.002935978816822171'\n",
      "  Person 60:\n",
      "   'boundingBox: {'x': 238, 'y': 303, 'w': 11, 'h': 8}'\n",
      "   'confidence: 0.002537169260904193'\n",
      "  Person 61:\n",
      "   'boundingBox: {'x': 153, 'y': 306, 'w': 17, 'h': 13}'\n",
      "   'confidence: 0.0025096095632761717'\n",
      "  Person 62:\n",
      "   'boundingBox: {'x': 538, 'y': 304, 'w': 40, 'h': 50}'\n",
      "   'confidence: 0.0023995782248675823'\n",
      "  Person 63:\n",
      "   'boundingBox: {'x': 0, 'y': 298, 'w': 6, 'h': 17}'\n",
      "   'confidence: 0.002120478544384241'\n",
      "  Person 64:\n",
      "   'boundingBox: {'x': 526, 'y': 298, 'w': 12, 'h': 14}'\n",
      "   'confidence: 0.0021035056561231613'\n",
      "  Person 65:\n",
      "   'boundingBox: {'x': 490, 'y': 301, 'w': 16, 'h': 14}'\n",
      "   'confidence: 0.0020975552033632994'\n",
      "  Person 66:\n",
      "   'boundingBox: {'x': 470, 'y': 302, 'w': 19, 'h': 50}'\n",
      "   'confidence: 0.0020265160128474236'\n",
      "  Person 67:\n",
      "   'boundingBox: {'x': 307, 'y': 305, 'w': 8, 'h': 18}'\n",
      "   'confidence: 0.001955267507582903'\n",
      "  Person 68:\n",
      "   'boundingBox: {'x': 271, 'y': 321, 'w': 15, 'h': 19}'\n",
      "   'confidence: 0.0019214395433664322'\n",
      "  Person 69:\n",
      "   'boundingBox: {'x': 514, 'y': 302, 'w': 15, 'h': 18}'\n",
      "   'confidence: 0.0019175410270690918'\n",
      "  Person 70:\n",
      "   'boundingBox: {'x': 505, 'y': 304, 'w': 10, 'h': 16}'\n",
      "   'confidence: 0.0018709327559918165'\n",
      "  Person 71:\n",
      "   'boundingBox: {'x': 204, 'y': 306, 'w': 14, 'h': 12}'\n",
      "   'confidence: 0.0016356038395315409'\n",
      "  Person 72:\n",
      "   'boundingBox: {'x': 498, 'y': 300, 'w': 10, 'h': 12}'\n",
      "   'confidence: 0.0015785599825903773'\n",
      "  Person 73:\n",
      "   'boundingBox: {'x': 40, 'y': 318, 'w': 20, 'h': 33}'\n",
      "   'confidence: 0.0015236876206472516'\n",
      "  Person 74:\n",
      "   'boundingBox: {'x': 450, 'y': 299, 'w': 9, 'h': 24}'\n",
      "   'confidence: 0.0013898690231144428'\n",
      "  Person 75:\n",
      "   'boundingBox: {'x': 19, 'y': 307, 'w': 14, 'h': 31}'\n",
      "   'confidence: 0.0013766487827524543'\n",
      "  Person 76:\n",
      "   'boundingBox: {'x': 0, 'y': 306, 'w': 20, 'h': 33}'\n",
      "   'confidence: 0.0013422933407127857'\n",
      "  Person 77:\n",
      "   'boundingBox: {'x': 58, 'y': 305, 'w': 10, 'h': 14}'\n",
      "   'confidence: 0.0012507765786722302'\n",
      "  Person 78:\n",
      "   'boundingBox: {'x': 0, 'y': 311, 'w': 16, 'h': 112}'\n",
      "   'confidence: 0.0011957151582464576'\n",
      "  Person 79:\n",
      "   'boundingBox: {'x': 369, 'y': 299, 'w': 11, 'h': 17}'\n",
      "   'confidence: 0.0011728907702490687'\n",
      "  Person 80:\n",
      "   'boundingBox: {'x': 376, 'y': 308, 'w': 14, 'h': 30}'\n",
      "   'confidence: 0.001118161715567112'\n",
      "  Person 81:\n",
      "   'boundingBox: {'x': 559, 'y': 298, 'w': 10, 'h': 15}'\n",
      "   'confidence: 0.0011024505365639925'\n",
      "  Person 82:\n",
      "   'boundingBox: {'x': 126, 'y': 303, 'w': 22, 'h': 28}'\n",
      "   'confidence: 0.001102263806387782'\n",
      "  Person 83:\n",
      "   'boundingBox: {'x': 599, 'y': 316, 'w': 14, 'h': 36}'\n",
      "   'confidence: 0.0010657792445272207'\n",
      "  Person 84:\n",
      "   'boundingBox: {'x': 293, 'y': 306, 'w': 13, 'h': 17}'\n",
      "   'confidence: 0.001013513538055122'\n",
      "\n",
      " Smart_crops:\n",
      "   'aspectRatio: 1.4'\n",
      "   'boundingBox: {'x': 29, 'y': 21, 'w': 642, 'h': 458}'\n",
      "\n",
      " Tags:\n",
      "  Tag 1:\n",
      "   'name: text'\n",
      "   'confidence: 0.9999241828918457'\n",
      "  Tag 2:\n",
      "   'name: car'\n",
      "   'confidence: 0.9932393431663513'\n",
      "  Tag 3:\n",
      "   'name: outdoor'\n",
      "   'confidence: 0.9884916543960571'\n",
      "  Tag 4:\n",
      "   'name: sky'\n",
      "   'confidence: 0.981989860534668'\n",
      "  Tag 5:\n",
      "   'name: vehicle'\n",
      "   'confidence: 0.9769678115844727'\n",
      "  Tag 6:\n",
      "   'name: land vehicle'\n",
      "   'confidence: 0.9640927910804749'\n",
      "  Tag 7:\n",
      "   'name: building'\n",
      "   'confidence: 0.9489233493804932'\n",
      "  Tag 8:\n",
      "   'name: billboard'\n",
      "   'confidence: 0.9373397827148438'\n",
      "  Tag 9:\n",
      "   'name: wheel'\n",
      "   'confidence: 0.8925918340682983'\n",
      "  Tag 10:\n",
      "   'name: advertising'\n",
      "   'confidence: 0.8603949546813965'\n",
      "  Tag 11:\n",
      "   'name: sign'\n",
      "   'confidence: 0.8481827974319458'\n",
      "  Tag 12:\n",
      "   'name: shop'\n",
      "   'confidence: 0.8277586698532104'\n",
      "  Tag 13:\n",
      "   'name: street'\n",
      "   'confidence: 0.6722654104232788'\n"
     ]
    }
   ],
   "source": [
    "# Get a caption for the image. This will be a synchronously (blocking) call.\n",
    "# Python ImageAnalysisClient.analyze_from_url method: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.imageanalysisclient?view=azure-python#azure-ai-vision-imageanalysis-imageanalysisclient-analyze-from-url\n",
    "#   Visual Features: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.visualfeatures?view=azure-python\n",
    "result = client.analyze(\n",
    "    image_data=image_data,\n",
    "    visual_features=[VisualFeatures.CAPTION, VisualFeatures.READ, VisualFeatures.DENSE_CAPTIONS, VisualFeatures.OBJECTS, VisualFeatures.PEOPLE, VisualFeatures.SMART_CROPS, VisualFeatures.TAGS]\n",
    "    #gender_neutral_caption=True,  # Optional (default is False)\n",
    ")\n",
    "\n",
    "# Python ImageAnalysisClient results: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models?view=azure-python\n",
    "print(\"Image analysis results:\")\n",
    "\n",
    "# # Python CaptionResult Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.captionresult?view=azure-python\n",
    "print(\"\\n Captions:\")\n",
    "if result.caption is not None:\n",
    "    # The '.4f' format specifier formats the confidence score to have 4 decimal places\n",
    "    print(f\"   '{result.caption.text}', Confidence {result.caption.confidence:.4f}\")\n",
    "\n",
    "# # Python DenseCaption Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.densecaption?view=azure-python#azure-ai-vision-imageanalysis-models-densecaption-values\n",
    "print(\"\\n Dense captions:\")\n",
    "if result.dense_captions is not None:\n",
    "    for v in result.dense_captions.values():        \n",
    "        for densecaption in v:\n",
    "            print(f\"   '{densecaption.text}', Confidence {densecaption.confidence:.4f}, {densecaption.bounding_box}\")\n",
    "\n",
    "# # Print text (OCR) analysis results to the console\n",
    "# # Python ReadResult Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.readresult?view=azure-python\n",
    "print(\"\\n Read-OCR:\")\n",
    "if result.read is not None:\n",
    "    # Python DetectedTextBlock Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.detectedtextblock?view=azure-python\n",
    "    # Python DetectedTextLine Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.detectedtextline?view=azure-python\n",
    "    # Python DetectedTextWord Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.detectedtextword?view=azure-python\n",
    "    for line in result.read.blocks[0].lines:\n",
    "        print(f\"   Line: '{line.text}', Bounding box {line.bounding_polygon}\")\n",
    "        for word in line.words:\n",
    "            print(f\"     Word: '{word.text}', Bounding polygon {word.bounding_polygon}, Confidence {word.confidence:.4f}\")\n",
    "\n",
    "# # Python ObjectsResult Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.objectsresult?view=azure-python#azure-ai-vision-imageanalysis-models-objectsresult-get\n",
    "# # Python DetectedObject Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.detectedobject?view=azure-python\n",
    "print(\"\\n Objects:\") \n",
    "if result.objects is not None:\n",
    "    for i, detectedobjects in enumerate(result.objects.get(\"values\")):\n",
    "        print(f\"  Object {i + 1}:\")\n",
    "        for k,v in detectedobjects.items():\n",
    "            print(f\"   '{k}: {v}'\")            \n",
    "\n",
    "# # Python PeopleResult Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.peopleresult?view=azure-python\n",
    "# # Python DetectedPerson Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.detectedperson?view=azure-python\n",
    "print(\"\\n People:\")\n",
    "if result.people is not None:\n",
    "    for i,detectedperson in enumerate(result.people.get(\"values\")):\n",
    "        print(f\"  Person {i + 1}:\")\n",
    "        for k,v in detectedperson.items():\n",
    "            print(f\"   '{k}: {v}'\")\n",
    "\n",
    "# # Python SmartCropsResult Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.smartcropsresult?view=azure-python\n",
    "# # Python CropRegion Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.cropregion?view=azure-python\n",
    "print(\"\\n Smart_crops:\")\n",
    "if result.smart_crops is not None:        \n",
    "    for smart_crops in result.smart_crops.get(\"values\"):\n",
    "        for k,v in smart_crops.items():\n",
    "            print(f\"   '{k}: {v}'\")\n",
    "\n",
    "# # Python TagsResult Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.tagsresult?view=azure-python\n",
    "# # Python DetectedTag Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.detectedtag?view=azure-python\n",
    "print(\"\\n Tags:\")\n",
    "if result.tags is not None:    \n",
    "    for i, detectedtags in enumerate(result.tags.get(\"values\")):\n",
    "        print(f\"  Tag {i + 1}:\")\n",
    "        for k,v in detectedtags.items():\n",
    "            print(f\"   '{k}: {v}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a caption for the image. This will be a synchronously (blocking) call.\n",
    "# Python ImageAnalysisClient.analyze_from_url method: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.imageanalysisclient?view=azure-python#azure-ai-vision-imageanalysis-imageanalysisclient-analyze-from-url\n",
    "#   Visual Features: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.visualfeatures?view=azure-python\n",
    "result = client.analyze(\n",
    "    image_data=image_data,\n",
    "    visual_features=[VisualFeatures.CAPTION, VisualFeatures.READ, VisualFeatures.DENSE_CAPTIONS, VisualFeatures.OBJECTS, VisualFeatures.PEOPLE, VisualFeatures.SMART_CROPS, VisualFeatures.TAGS]\n",
    "    #gender_neutral_caption=True,  # Optional (default is False)\n",
    ")\n",
    "\n",
    "# Python ImageAnalysisClient results: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models?view=azure-python\n",
    "print(\"Image analysis results:\")\n",
    "\n",
    "# # Python CaptionResult Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.captionresult?view=azure-python\n",
    "print(\"\\n Captions:\")\n",
    "if result.caption is not None:\n",
    "    # The '.4f' format specifier formats the confidence score to have 4 decimal places\n",
    "    print(f\"   '{result.caption.text}', Confidence {result.caption.confidence:.4f}\")\n",
    "\n",
    "# # Python DenseCaption Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.densecaption?view=azure-python#azure-ai-vision-imageanalysis-models-densecaption-values\n",
    "print(\"\\n Dense captions:\")\n",
    "if result.dense_captions is not None:\n",
    "    for v in result.dense_captions.values():        \n",
    "        for densecaption in v:\n",
    "            print(f\"   '{densecaption.text}', Confidence {densecaption.confidence:.4f}, {densecaption.bounding_box}\")\n",
    "\n",
    "# # Print text (OCR) analysis results to the console\n",
    "# # Python ReadResult Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.readresult?view=azure-python\n",
    "print(\"\\n Read-OCR:\")\n",
    "if result.read is not None:\n",
    "    # Python DetectedTextBlock Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.detectedtextblock?view=azure-python\n",
    "    # Python DetectedTextLine Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.detectedtextline?view=azure-python\n",
    "    # Python DetectedTextWord Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.detectedtextword?view=azure-python\n",
    "    for line in result.read.blocks[0].lines:\n",
    "        print(f\"   Line: '{line.text}', Bounding box {line.bounding_polygon}\")\n",
    "        for word in line.words:\n",
    "            print(f\"     Word: '{word.text}', Bounding polygon {word.bounding_polygon}, Confidence {word.confidence:.4f}\")\n",
    "\n",
    "# # Python ObjectsResult Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.objectsresult?view=azure-python#azure-ai-vision-imageanalysis-models-objectsresult-get\n",
    "# # Python DetectedObject Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.detectedobject?view=azure-python\n",
    "print(\"\\n Objects:\") \n",
    "if result.objects is not None:\n",
    "    for i, detectedobjects in enumerate(result.objects.get(\"values\")):\n",
    "        print(f\"  Object {i + 1}:\")\n",
    "        for k,v in detectedobjects.items():\n",
    "            print(f\"   '{k}: {v}'\")            \n",
    "\n",
    "# # Python PeopleResult Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.peopleresult?view=azure-python\n",
    "# # Python DetectedPerson Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.detectedperson?view=azure-python\n",
    "print(\"\\n People:\")\n",
    "if result.people is not None:\n",
    "    for i,detectedperson in enumerate(result.people.get(\"values\")):\n",
    "        print(f\"  Person {i + 1}:\")\n",
    "        for k,v in detectedperson.items():\n",
    "            print(f\"   '{k}: {v}'\")\n",
    "\n",
    "# # Python SmartCropsResult Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.smartcropsresult?view=azure-python\n",
    "# # Python CropRegion Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.cropregion?view=azure-python\n",
    "print(\"\\n Smart_crops:\")\n",
    "if result.smart_crops is not None:        \n",
    "    for smart_crops in result.smart_crops.get(\"values\"):\n",
    "        for k,v in smart_crops.items():\n",
    "            print(f\"   '{k}: {v}'\")\n",
    "\n",
    "# # Python TagsResult Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.tagsresult?view=azure-python\n",
    "# # Python DetectedTag Class: https://learn.microsoft.com/en-us/python/api/azure-ai-vision-imageanalysis/azure.ai.vision.imageanalysis.models.detectedtag?view=azure-python\n",
    "print(\"\\n Tags:\")\n",
    "if result.tags is not None:    \n",
    "    for i, detectedtags in enumerate(result.tags.get(\"values\")):\n",
    "        print(f\"  Tag {i + 1}:\")\n",
    "        for k,v in detectedtags.items():\n",
    "            print(f\"   '{k}: {v}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
