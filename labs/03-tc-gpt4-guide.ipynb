{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract image content with Azure AI Chat-GPT4o\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "* [Azure OpenAI Service](https://azure.microsoft.com/en-us/products/ai-services/openai-service)\n",
    "  * GPT-4o\n",
    "* Python environment, version 3.10 or higher\n",
    "* GitHub CodeSpaces\n",
    "* Visual Studio Code\n",
    "  * Extensions: Python and Jupyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python packages\n",
    "# ! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import Utility Functions\n",
    "from utils import (\n",
    "    word_wrap,\n",
    "    local_image_to_data_url    \n",
    ")\n",
    "\n",
    "# OpenAI Python libraries\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load variables\n",
    "load_dotenv()\n",
    "\n",
    "# Variables - Azure Services\n",
    "AZURE_OPENAI_ACCOUNT=os.environ[\"AZURE_OPENAI_ACCOUNT\"]\n",
    "AZURE_OPENAI_KEY=os.environ[\"AZURE_OPENAI_KEY\"]\n",
    "\n",
    "# Variables - Names\n",
    "azure_openai_gpt4o_name=\"gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure OpenAI Call\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AzureOpenAI call using a local image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an OpenAI client object\n",
    "# Python AzureOpenAI Class: https://github.com/openai/openai-python?tab=readme-ov-file#microsoft-azure-openai\n",
    "openai_client=AzureOpenAI(\n",
    "     api_version=\"2024-06-01\",\n",
    "     azure_endpoint=AZURE_OPENAI_ACCOUNT,     \n",
    "     api_key=AZURE_OPENAI_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AzureOpenAI call using a a Local Image\n",
    "\n",
    "1. First convert the image file to base64 so it can be passed to the API\n",
    "1. Send the base64 file to Azure OpenAI API using the image_url field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add and test local image 1\n",
    "image_path='../images-lab-tests/seattle-pikeplace-1.jpg'\n",
    "data_url=local_image_to_data_url(image_path)\n",
    "#print(\"Data URL:\", data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure OpenAI message content only:\n",
      "**Brief Description:**  \n",
      "The image depicts the iconic Pike Place Market in Seattle, Washington, a\n",
      "bustling and historic marketplace known for its vibrant atmosphere and local vendors. The scene\n",
      "features wet, reflective brick pavement, parked cars, and people walking under overcast skies,\n",
      "adding to the character of the urban setting.\n",
      "\n",
      "**Entities:**  \n",
      "- Location: Seattle, Washington  \n",
      "-\n",
      "Landmark: Pike Place Market  \n",
      "- Key Features: Public Market Center sign, Farmers Market sign, \"Meet\n",
      "the Producers\" text on the building, a clock on the signage, people, cars, and storefronts.\n",
      "\n",
      "**Text\n",
      "in the Image:**  \n",
      "- PUBLIC MARKET CENTER  \n",
      "- FARMERS MARKET  \n",
      "- MEET THE PRODUCERS  \n",
      "- Seattle,\n",
      "Washington  \n",
      "\n",
      "**Compiled Paragraph:**  \n",
      "This image showcases the famous Pike Place Market in\n",
      "Seattle, Washington, under a cloudy sky with reflective, wet brick pavement. The main focus is the\n",
      "\"Public Market Center\" sign with its red lettering and iconic clock, flanked by a \"Farmers Market\"\n",
      "entryway. Above the building, the phrase \"Meet the Producers\" is prominently displayed, capturing\n",
      "the emphasis on local, fresh produce and goods. Several parked cars and people exploring the area\n",
      "add life to the scene, enhancing the market's lively and historic appeal.\n"
     ]
    }
   ],
   "source": [
    "response=openai_client.chat.completions.create(\n",
    "    model=azure_openai_gpt4o_name,\n",
    "    messages=[\n",
    "        { \n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are a Visual Cognitive system tasked with extracting text and information from images.\" \n",
    "        },\n",
    "        { \n",
    "            \"role\": \"user\", \n",
    "            \"content\": [  \n",
    "                { \n",
    "                    \"type\": \"text\", \n",
    "                    \"text\": \"Extract all the following data in different sections from the provided \\\n",
    "                        image and compile it into a paragraph: Brief description, Entities, and Text in the image.\"\n",
    "                    \n",
    "                },\n",
    "                { \n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": data_url                        \n",
    "                    }\n",
    "                }\n",
    "            ] \n",
    "        } \n",
    "    ],\n",
    "    #  Set a \"max_tokens\" value, or the return output will be cut off\n",
    "    max_tokens=3000 \n",
    ")\n",
    "\n",
    "print(f\"Azure OpenAI message content only:\\n{word_wrap(response.choices[0].message.content)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AzureOpenAI call using a URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the image URL\n",
    "url = 'https://learn.microsoft.com/azure/ai-services/computer-vision/media/quickstarts/presentation.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure OpenAI message content only:\n",
      "**Brief Description:**  \n",
      "The image depicts an interactive meeting or collaboration setup featuring\n",
      "a person using a smart touchscreen display. The screen shows a calendar interface for scheduling\n",
      "and joining meetings in a professional environment.\n",
      "\n",
      "**Tags:**  \n",
      "Technology, Interactive Display,\n",
      "Collaboration, Smart Screen, Meeting Schedule, Office, Workspace, User Interaction.\n",
      "\n",
      "**Text in the\n",
      "Image:**  \n",
      "_Upper left corner:_  \n",
      "- **9:35 AM**  \n",
      "- Conference room | 54684554  \n",
      "- 555-123-4567 \n",
      "\n",
      "\n",
      "_Overlay on the right panel (Meeting schedule details):_  \n",
      "- **Town Hall**  \n",
      "  9:00 AM - 10:00 AM\n",
      " \n",
      "  Aaron Buxton  \n",
      "  [Join]  \n",
      "\n",
      "- **Daily SCRUM**  \n",
      "  10:00 AM - 10:30 AM  \n",
      "  Charlotte De Coutun \n",
      "\n",
      "\n",
      "- **Quarterly All-Hands**  \n",
      "  11:00 AM - 12:00 PM  \n",
      "  Satish Sharma  \n",
      "\n",
      "- **Weekly stand-up**  \n",
      " \n",
      "12:00 PM - 12:30 PM  \n",
      "  Danielle Marcario  \n",
      "\n",
      "- **Product review**  \n",
      "\n",
      "_Horizontal navigation buttons\n",
      "(icons):_  \n",
      "- Call, Chat, Email, Whiteboard, Share, More.\n"
     ]
    }
   ],
   "source": [
    "#query=\"Extract all the text from the provided image and compile it into a paragraph.\"\n",
    "\n",
    "response=openai_client.chat.completions.create(\n",
    "    model=azure_openai_gpt4o_name,\n",
    "    messages=[\n",
    "        { \n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are a Visual Cognitive system tasked with extracting text and information from images.\" \n",
    "        },\n",
    "        { \n",
    "            \"role\": \"user\", \n",
    "            \"content\": [  \n",
    "                { \n",
    "                    \"type\": \"text\", \n",
    "                    \"text\": \"Using the image provided by image_url, get the following information: \\\n",
    "                        Brief description, Tags, and Text in the image.\"\n",
    "                    \n",
    "                },\n",
    "                { \n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": url                        \n",
    "                    }\n",
    "                }\n",
    "            ] \n",
    "        } \n",
    "    ],\n",
    "    #  Set a \"max_tokens\" value, or the return output will be cut off\n",
    "    max_tokens=3000 \n",
    ")\n",
    "\n",
    "print(f\"Azure OpenAI message content only:\\n{word_wrap(response.choices[0].message.content)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze another image\n",
    "\n",
    "Try using an image from the [images-lab-tests](../images-lab-tests/) folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add and test local image\n",
    "image_path='../images-lab-tests/<REPLACE_WITH_IMAGE_FILE>'\n",
    "data_url=local_image_to_data_url(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=openai_client.chat.completions.create(\n",
    "    model=azure_openai_gpt4o_name,\n",
    "    messages=[\n",
    "        { \n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are a Visual Cognitive system tasked with extracting text and information from images.\" \n",
    "        },\n",
    "        { \n",
    "            \"role\": \"user\", \n",
    "            \"content\": [  \n",
    "                { \n",
    "                    \"type\": \"text\", \n",
    "                    \"text\": \"Extract all the following data in different sections from the provided \\\n",
    "                        image and compile it into a paragraph: Brief description, Entities, and Text in the image.\"\n",
    "                    \n",
    "                },\n",
    "                { \n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": data_url                        \n",
    "                    }\n",
    "                }\n",
    "            ] \n",
    "        } \n",
    "    ],\n",
    "    #  Set a \"max_tokens\" value, or the return output will be cut off\n",
    "    max_tokens=3000 \n",
    ")\n",
    "\n",
    "print(f\"Azure OpenAI message content only:\\n{word_wrap(response.choices[0].message.content)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to answer the following questions\n",
    "\n",
    "* What output formats would you use in a real production application?\n",
    "* In which Azure service would you host this code?\n",
    "* What is the difference between OCR, Image Analysis and GPT-4o?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
